{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will be creating a model to predict game results based off of games metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOME_TEAM_ABBREV</th>\n",
       "      <th>AWAY_TEAM_ABBREV</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>HOME_RECORD</th>\n",
       "      <th>ROAD_RECORD</th>\n",
       "      <th>HOME_TEAM_WINS</th>\n",
       "      <th>OFF_HOME_TEAM</th>\n",
       "      <th>DEF_HOME_TEAM</th>\n",
       "      <th>REB_HOME_TEAM</th>\n",
       "      <th>OFF_AWAY_TEAM</th>\n",
       "      <th>...</th>\n",
       "      <th>REB_AWAY_TEAM</th>\n",
       "      <th>OFF_HOME_RATING</th>\n",
       "      <th>DEF_HOME_RATING</th>\n",
       "      <th>REB_HOME_RATING</th>\n",
       "      <th>OFF_AWAY_RATING</th>\n",
       "      <th>DEF_AWAY_RATING</th>\n",
       "      <th>REB_AWAY_RATING</th>\n",
       "      <th>OFFENSE_DIFFERENCE</th>\n",
       "      <th>DEFENSE_DIFFERENCE</th>\n",
       "      <th>REBOUND_DIFFERENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CHA</td>\n",
       "      <td>MIL</td>\n",
       "      <td>2019</td>\n",
       "      <td>-10</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>[31.006061923099047, 38.54431945273706, 32.704...</td>\n",
       "      <td>[22.10810810810811, 28.81081081081081, 22.2162...</td>\n",
       "      <td>[23.308831973407106, 31.182094864883947, 35.85...</td>\n",
       "      <td>[35.22876717660929, 65.69235449876932, 37.7094...</td>\n",
       "      <td>...</td>\n",
       "      <td>[17.28106695654748, 58.916993867334874, 24.020...</td>\n",
       "      <td>33.477084</td>\n",
       "      <td>20.021622</td>\n",
       "      <td>26.029330</td>\n",
       "      <td>39.084703</td>\n",
       "      <td>26.043243</td>\n",
       "      <td>26.156013</td>\n",
       "      <td>-5.607619</td>\n",
       "      <td>-6.021622</td>\n",
       "      <td>-0.126683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>MIN</td>\n",
       "      <td>DAL</td>\n",
       "      <td>2019</td>\n",
       "      <td>-15</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[31.63000087955722, 24.95066073256286, 31.9678...</td>\n",
       "      <td>[29.405405405405403, 5.351351351351353, 21.459...</td>\n",
       "      <td>[17.25898881532161, 13.456670998027207, 24.888...</td>\n",
       "      <td>[41.62244146926659, 29.710952103096684, 48.975...</td>\n",
       "      <td>...</td>\n",
       "      <td>[22.37412606063427, 21.685969263523607, 38.388...</td>\n",
       "      <td>35.712786</td>\n",
       "      <td>20.486486</td>\n",
       "      <td>23.266696</td>\n",
       "      <td>36.429985</td>\n",
       "      <td>20.762162</td>\n",
       "      <td>21.562970</td>\n",
       "      <td>-0.717199</td>\n",
       "      <td>-0.275676</td>\n",
       "      <td>1.703727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>LAC</td>\n",
       "      <td>PHI</td>\n",
       "      <td>2019</td>\n",
       "      <td>19</td>\n",
       "      <td>-13</td>\n",
       "      <td>1</td>\n",
       "      <td>[44.03598551522123, 37.12656298881634, 28.6805...</td>\n",
       "      <td>[53.513513513513516, 14.702702702702705, 8.054...</td>\n",
       "      <td>[26.533392671106416, 30.47002411684483, 18.105...</td>\n",
       "      <td>[25.93974282213501, 42.6812218695769, 44.48622...</td>\n",
       "      <td>...</td>\n",
       "      <td>[9.964396462598044, 30.33662390755899, 42.5476...</td>\n",
       "      <td>40.421789</td>\n",
       "      <td>25.940541</td>\n",
       "      <td>25.653422</td>\n",
       "      <td>34.188720</td>\n",
       "      <td>22.178378</td>\n",
       "      <td>21.569685</td>\n",
       "      <td>6.233069</td>\n",
       "      <td>3.762162</td>\n",
       "      <td>4.083737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DEN</td>\n",
       "      <td>TOR</td>\n",
       "      <td>2019</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>[43.82752587784179, 32.27739138834381, 54.2482...</td>\n",
       "      <td>[28.108108108108105, 21.513513513513516, 34.81...</td>\n",
       "      <td>[29.068883152536877, 24.15430293073206, 63.988...</td>\n",
       "      <td>[27.52057045795501, 34.03891798056395, 45.9075...</td>\n",
       "      <td>...</td>\n",
       "      <td>[14.924055357398997, 26.57847134410255, 40.123...</td>\n",
       "      <td>36.647265</td>\n",
       "      <td>22.421622</td>\n",
       "      <td>24.336735</td>\n",
       "      <td>33.382946</td>\n",
       "      <td>19.313514</td>\n",
       "      <td>19.538902</td>\n",
       "      <td>3.264319</td>\n",
       "      <td>3.108108</td>\n",
       "      <td>4.797833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>SAC</td>\n",
       "      <td>DET</td>\n",
       "      <td>2019</td>\n",
       "      <td>-2</td>\n",
       "      <td>-13</td>\n",
       "      <td>1</td>\n",
       "      <td>[45.720084398700706, 29.937134564603657, 36.76...</td>\n",
       "      <td>[14.702702702702705, 16.702702702702705, 15.40...</td>\n",
       "      <td>[34.7626909366028, 23.82034569793837, 23.35391...</td>\n",
       "      <td>[27.664451648755787, 32.20682601470979, 37.932...</td>\n",
       "      <td>...</td>\n",
       "      <td>[10.074796140113614, 23.019953413609556, 41.90...</td>\n",
       "      <td>39.696424</td>\n",
       "      <td>22.264865</td>\n",
       "      <td>25.286163</td>\n",
       "      <td>30.492550</td>\n",
       "      <td>15.410811</td>\n",
       "      <td>17.782184</td>\n",
       "      <td>9.203875</td>\n",
       "      <td>6.854054</td>\n",
       "      <td>7.503980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  HOME_TEAM_ABBREV AWAY_TEAM_ABBREV  SEASON  HOME_RECORD  ROAD_RECORD  \\\n",
       "0              CHA              MIL    2019          -10           20   \n",
       "1              MIN              DAL    2019          -15           11   \n",
       "2              LAC              PHI    2019           19          -13   \n",
       "3              DEN              TOR    2019           17           10   \n",
       "4              SAC              DET    2019           -2          -13   \n",
       "\n",
       "   HOME_TEAM_WINS                                      OFF_HOME_TEAM  \\\n",
       "0               0  [31.006061923099047, 38.54431945273706, 32.704...   \n",
       "1               0  [31.63000087955722, 24.95066073256286, 31.9678...   \n",
       "2               1  [44.03598551522123, 37.12656298881634, 28.6805...   \n",
       "3               1  [43.82752587784179, 32.27739138834381, 54.2482...   \n",
       "4               1  [45.720084398700706, 29.937134564603657, 36.76...   \n",
       "\n",
       "                                       DEF_HOME_TEAM  \\\n",
       "0  [22.10810810810811, 28.81081081081081, 22.2162...   \n",
       "1  [29.405405405405403, 5.351351351351353, 21.459...   \n",
       "2  [53.513513513513516, 14.702702702702705, 8.054...   \n",
       "3  [28.108108108108105, 21.513513513513516, 34.81...   \n",
       "4  [14.702702702702705, 16.702702702702705, 15.40...   \n",
       "\n",
       "                                       REB_HOME_TEAM  \\\n",
       "0  [23.308831973407106, 31.182094864883947, 35.85...   \n",
       "1  [17.25898881532161, 13.456670998027207, 24.888...   \n",
       "2  [26.533392671106416, 30.47002411684483, 18.105...   \n",
       "3  [29.068883152536877, 24.15430293073206, 63.988...   \n",
       "4  [34.7626909366028, 23.82034569793837, 23.35391...   \n",
       "\n",
       "                                       OFF_AWAY_TEAM  ...  \\\n",
       "0  [35.22876717660929, 65.69235449876932, 37.7094...  ...   \n",
       "1  [41.62244146926659, 29.710952103096684, 48.975...  ...   \n",
       "2  [25.93974282213501, 42.6812218695769, 44.48622...  ...   \n",
       "3  [27.52057045795501, 34.03891798056395, 45.9075...  ...   \n",
       "4  [27.664451648755787, 32.20682601470979, 37.932...  ...   \n",
       "\n",
       "                                       REB_AWAY_TEAM OFF_HOME_RATING  \\\n",
       "0  [17.28106695654748, 58.916993867334874, 24.020...       33.477084   \n",
       "1  [22.37412606063427, 21.685969263523607, 38.388...       35.712786   \n",
       "2  [9.964396462598044, 30.33662390755899, 42.5476...       40.421789   \n",
       "3  [14.924055357398997, 26.57847134410255, 40.123...       36.647265   \n",
       "4  [10.074796140113614, 23.019953413609556, 41.90...       39.696424   \n",
       "\n",
       "   DEF_HOME_RATING  REB_HOME_RATING  OFF_AWAY_RATING  DEF_AWAY_RATING  \\\n",
       "0        20.021622        26.029330        39.084703        26.043243   \n",
       "1        20.486486        23.266696        36.429985        20.762162   \n",
       "2        25.940541        25.653422        34.188720        22.178378   \n",
       "3        22.421622        24.336735        33.382946        19.313514   \n",
       "4        22.264865        25.286163        30.492550        15.410811   \n",
       "\n",
       "   REB_AWAY_RATING  OFFENSE_DIFFERENCE  DEFENSE_DIFFERENCE  REBOUND_DIFFERENCE  \n",
       "0        26.156013           -5.607619           -6.021622           -0.126683  \n",
       "1        21.562970           -0.717199           -0.275676            1.703727  \n",
       "2        21.569685            6.233069            3.762162            4.083737  \n",
       "3        19.538902            3.264319            3.108108            4.797833  \n",
       "4        17.782184            9.203875            6.854054            7.503980  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "games = pd.read_pickle(os.path.join('Data','final_games.pkl'))\n",
    "games.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I create tables of the parameters and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = games[['HOME_RECORD', 'ROAD_RECORD', 'OFF_HOME_RATING', 'DEF_HOME_RATING', 'REB_HOME_RATING',\n",
    "           'OFF_AWAY_RATING', 'DEF_AWAY_RATING', 'REB_AWAY_RATING','OFFENSE_DIFFERENCE', \n",
    "           'DEFENSE_DIFFERENCE','REBOUND_DIFFERENCE']]\n",
    "y = games['HOME_TEAM_WINS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HOME_RECORD             int64\n",
       "ROAD_RECORD             int64\n",
       "OFF_HOME_RATING       float64\n",
       "DEF_HOME_RATING       float64\n",
       "REB_HOME_RATING       float64\n",
       "OFF_AWAY_RATING       float64\n",
       "DEF_AWAY_RATING       float64\n",
       "REB_AWAY_RATING       float64\n",
       "OFFENSE_DIFFERENCE    float64\n",
       "DEFENSE_DIFFERENCE    float64\n",
       "REBOUND_DIFFERENCE    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classifier I used to make the model is the Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression( solver = 'lbfgs')\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has a score of around 70% which is signifigantly better than guessing, but I hoped for a model with at least a 80% score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.725717776420281"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the average percent accuracy I will take a couple of samples and average the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_score():\n",
    "    \"\"\" returns score of one trial \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    m = LogisticRegression( solver = 'lbfgs')\n",
    "    m.fit(X_train, y_train)\n",
    "    return m.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_average(n):\n",
    "    \"\"\" returns average score \"\"\"\n",
    "    return np.array([return_score() for i in range(n)]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.721930360415394"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_average(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows that our model struggles more with guessing negative or losses for the home team as the false positive score is too high for comfort. This could be a result of actual upsets that we have been seeing in the past year or it could be because our model is overfitting. Still if I was a betting man ( I'm not saying I am) an around 70% accruacy score is not comforting. \n",
    "\n",
    "Looking at our diagnostics, I am concerned with the accruacy score of this classifier as it is too low for comfort. Specifically, the specificity( chance that the machine predicts the home team losing) is too low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(436, 253, 196, 752)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test, pred).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7482587064676617\n",
      "0.7932489451476793\n",
      "0.725717776420281\n",
      "0.7700972862263185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6328011611030478"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##amount of diagnosed home wins that were actually home wins\n",
    "print(metrics.precision_score(y_test,pred))\n",
    "##amount of positives home wins that was actually diagnosed correctly\n",
    "print(metrics.recall_score(y_test,pred))\n",
    "## How many times does our model correctly predict the outcome\n",
    "print(metrics.accuracy_score(y_test,pred))\n",
    "## measure of how well a test labels home wins\n",
    "print(metrics.f1_score(y_test,pred))\n",
    "\n",
    "## specificity\n",
    "tn / (tn + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll try a random forest classifier so that I can look at important features and also see what happens if I change the parameters. Currently we get again an around 70% accuracy score, but that is with default paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "guess = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7263286499694563"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a grid search to find the best parameters I could be using for the random forest classifier. As you can see the best parameters are {'max_depth': None, 'min_samples_leaf': 20, 'min_samples_split': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'max_depth': [2,3,4,5,7,10,13,15,18,None], \n",
    "    'min_samples_split':[2,3,5,7,10,15,20],\n",
    "    'min_samples_leaf':[2,3,5,7,10,15,20,22]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(RandomForestClassifier(), parameters, cv=5, scoring = 'accuracy')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(max_depth = 18, min_samples_leaf = 22, min_samples_split = 3)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that our random forest classifier dos a little better than our logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test, pred).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets get the average score again with the same parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_score():\n",
    "    \"\"\" returns score of one trial \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    m = RandomForestClassifier( min_samples_leaf = 20, min_samples_split = 7)\n",
    "    m.fit(X_train, y_train)\n",
    "    return m.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_average(n):\n",
    "    \"\"\" returns average score \"\"\"\n",
    "    return np.array([return_score() for i in range(n)]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_average(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to look at what features are the most important. It seems that the rebounding categories currently are least significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model.feature_importances_, X.columns.values, columns=['importance']).sort_values('importance', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of the rebounding parameters first, as you can see it does improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = games[['HOME_RECORD', 'ROAD_RECORD', 'OFF_HOME_RATING', 'DEF_HOME_RATING',\n",
    "           'OFF_AWAY_RATING', 'DEF_AWAY_RATING','OFFENSE_DIFFERENCE', \n",
    "           'DEFENSE_DIFFERENCE']]\n",
    "y = games['HOME_TEAM_WINS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "model = RandomForestClassifier( min_samples_leaf = 20, min_samples_split = 7)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I know these parameters and features are the best let's check the confusion matrix of this model to see if it performs better than a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = games[['HOME_RECORD', 'ROAD_RECORD', 'OFF_HOME_RATING', 'DEF_HOME_RATING',\n",
    "           'OFF_AWAY_RATING', 'DEF_AWAY_RATING','OFFENSE_DIFFERENCE', \n",
    "           'DEFENSE_DIFFERENCE']]\n",
    "y = games['HOME_TEAM_WINS']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "model = RandomForestClassifier( min_samples_leaf = 20, min_samples_split = 7)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the confusion matrix for a random forest classifier is slighlty better than the classifier of a logistic regression as the accruacy score is slightly higher. However, the specificity is still low, I realized that its probably because there are a good amount of upsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test, pred).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##amount of diagnosed home wins that were actually home wins\n",
    "print(metrics.precision_score(y_test,pred))\n",
    "##amount of positives home wins that was actually diagnosed correctly\n",
    "print(metrics.recall_score(y_test,pred))\n",
    "## How many times does our model correctly predict the outcome\n",
    "print(metrics.accuracy_score(y_test,pred))\n",
    "## measure of how well a test labels home wins\n",
    "print(metrics.f1_score(y_test,pred))\n",
    "\n",
    "## specificity\n",
    "tn / (tn + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_average(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KN Neighbors Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a KN Neighbors Classifier model and fit the model with a similar training and test set. The baseline model gives me around the same accuracy but let's do a grid search again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "guess = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KN_return_score():\n",
    "    \"\"\" returns score of one trial \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    m = KNeighborsClassifier()\n",
    "    m.fit(X_train, y_train)\n",
    "    return m.score(X_test, y_test)\n",
    "\n",
    "def KN_return_average(n):\n",
    "    \"\"\" returns average score \"\"\"\n",
    "    return np.array([KN_return_score() for i in range(n)]).mean()\n",
    "\n",
    "KN_return_average(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I do a grid search on the n_neighbors parameter of my KN Neighbors calssifier and find that the best parameter is 15 neighbors, so I use it and fit a new model against the test. As you can see it gives me a model that is simlar to the random forest model. So, I'm convinced that I probably need more features to make this model better... or there are just simply a lot of upsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'n_neighbors': [3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(KNeighborsClassifier(), parameters, cv=5, scoring = 'accuracy')\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 15)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KN_return_score():\n",
    "    \"\"\" returns score of one trial \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    m = KNeighborsClassifier(n_neighbors = 15)\n",
    "    m.fit(X_train, y_train)\n",
    "    return m.score(X_test, y_test)\n",
    "return_average(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the confusion matrix I get an accuracy similar to the one I get for a random forest classifier showing me that my model is probably as good as it can ever be. The specificity socre is low and that tells me that there are quite a lot of upsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test, pred).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##amount of diagnosed home wins that were actually home wins\n",
    "print(metrics.precision_score(y_test,pred))\n",
    "##amount of positives home wins that was actually diagnosed correctly\n",
    "print(metrics.recall_score(y_test,pred))\n",
    "## How many times does our model correctly predict the outcome\n",
    "print(metrics.accuracy_score(y_test,pred))\n",
    "## measure of how well a test labels home wins\n",
    "print(metrics.f1_score(y_test,pred))\n",
    "\n",
    "## specificity\n",
    "tn / (tn + fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
